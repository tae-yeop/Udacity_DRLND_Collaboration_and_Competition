{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.random.randn(4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_list(a):\n",
    "    return list(map(list, zip(*a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, obs_full = transpose_list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.8180854546974097,\n",
       " -1.0157692805039105,\n",
       " -2.011725628616091,\n",
       " 0.025155313782145985]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.2671366128842854,\n",
       " -0.7693943524038026,\n",
       " 1.6844372794932774,\n",
       " 0.4098731200740571]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is already saved in the Workspace and can be accessed at the file path provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.65278625 -1.5        -0.          0.\n",
      "  6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each actionS\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info.local_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info.rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 24)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: 0.04500000085681677\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):                                         # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info.previous_text_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.],\n",
       "       [ 0.,  0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info.previous_vector_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as I\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "        fan_in = layer.weight.data.size()[0]\n",
    "        lim = 1. / np.sqrt(fan_in)\n",
    "        return (-lim, lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=96, fc2_units=96):\n",
    "        \"\"\"\n",
    "        Build model and Intialize it\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int) : State space size\n",
    "            action_size (int) : Action space size\n",
    "            seed (int) : Random seed\n",
    "            fc1_unit (int)  \n",
    "            fc2_unit (int)\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize parameters of the layers\n",
    "        xavier_normal is used.\n",
    "        See \"Understanding the difficulty of training deep feedforward neural networks\" - Glorot, X. & Bengio, Y. (2010) for details.\n",
    "        \"\"\"\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 I.xavier_normal_(m.weight)\n",
    "\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))  # * is for unpacking list or tuple\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass state -> action\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=96, fc2_units=96, num_agents=2):\n",
    "        \"\"\"\n",
    "        Build model and Intialize it\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int) : State space size\n",
    "            action_size (int) : Action space size\n",
    "            seed (int) : Random seed\n",
    "            fc1_unit (int) : default 48, since num_agetns*state_size = 48\n",
    "            fc2_unit (int) \n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm1d(state_size)\n",
    "        # self.fc1 = nn.Linear(state_size*num_agents, fc1_units)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        # self.fc2 = nn.Linear(fc1_units+action_size*num_agents, fc2_units)\n",
    "        self.fc2 = nn.Linear(fc1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize parameters of the layers\n",
    "        xavier_normal is used.\n",
    "        See \"Understanding the difficulty of training deep feedforward neural networks\" - Glorot, X. & Bengio, Y. (2010) for details.\n",
    "        \"\"\"\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 I.xavier_normal_(m.weight)\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))  # * is for unpacking list or tuple\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "                \n",
    "    def forward(self, states, actions):\n",
    "        \"\"\"\n",
    "        Forward pass state, action -> q_value \n",
    "        \n",
    "        Params\n",
    "        =======\n",
    "            all_states (torch tensor) [batch_size, num_agents, state_size]\n",
    "            action_collections (torch tensor) [batch_size, num_agents, action_size]\n",
    "        Retunrs\n",
    "        ======\n",
    "            x (torch tensor) [batch_size, 1]\n",
    "        \"\"\"\n",
    "#         #[batch_size, num_agents, state_size] -> [batch_size, num_agents*state_size]\n",
    "#         states_flatten = all_states.view(all_states.shape[0], -1)\n",
    "        \n",
    "#         x = F.relu(self.fc1(states_flatten))\n",
    "#         # action_collections : [batch_size, num_agents, action_size] -> [batch_size, num_agents*action_size]\n",
    "#         actions_flatten = action_collections.view(action_collections.shape[0],-1)\n",
    "#         # concat \n",
    "#         x = torch.cat((actions_flatten,x), dim=1).to(device)\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "        x = self.bn0(states)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(torch.cat((x, actions.to(device)),dim=1))\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "class OUNoise():\n",
    "    def __init__(self, action_size, seed, mu=.0, theta=0.15, sigma=0.15):\n",
    "        \"\"\"\n",
    "        Set initial random process state.\n",
    "        \n",
    "        Params\n",
    "        =====\n",
    "            action_size (int)\n",
    "            seed (int) : For determinsitc random process. \n",
    "            mu (float) : center that noise will move around.\n",
    "            theta(flaot)\n",
    "            sigma(float)\n",
    "        \"\"\"\n",
    "        #self.noise_state = np.ones(action_size)*mu\n",
    "        self.mu = np.ones(action_size)*mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        np.random.seed(seed)\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the noise state to the mu.\n",
    "        \"\"\"\n",
    "        self.noise_state = self.mu\n",
    "        #self.noise_state= copy.copy(self.mu)\n",
    "    def sample(self):\n",
    "        x = self.noise_state\n",
    "        dx = self.theta*(self.mu-x) + self.sigma*(np.random.randn(len(x)))\n",
    "        self.noise_state = x+ dx\n",
    "        return self.noise_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, max_buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=max_buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\n",
    "            typename=\"Experience\",\n",
    "            field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)  # REVIEW: Does not remove from buffer?\n",
    "\n",
    "        states = np.vstack(\n",
    "            [transition.state for transition in experiences\n",
    "             if transition is not None])\n",
    "        actions = np.vstack(\n",
    "            [transition.action for transition in experiences\n",
    "             if transition is not None])\n",
    "        rewards = np.vstack(\n",
    "            [transition.reward for transition in experiences\n",
    "             if transition is not None])\n",
    "        next_states = np.vstack(\n",
    "            [transition.next_state for transition in experiences\n",
    "             if transition is not None])\n",
    "        dones = np.vstack(\n",
    "            [transition.done for transition in experiences\n",
    "             if transition is not None]).astype(np.uint8)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# class ReplayBuffer():\n",
    "    \n",
    "#     def __init__(self, buf_size, batch_size,seed):\n",
    "#         \"\"\"\n",
    "#         Set memory and bacth_size\n",
    "        \n",
    "#         Params\n",
    "#         =====\n",
    "#             buf_size (int) \n",
    "#             batch_size (int)\n",
    "#             seed (int)\n",
    "#         \"\"\"\n",
    "#         self.memory = deque(maxlen=buf_size)\n",
    "#         self.batch_size = batch_size\n",
    "#         np.random.seed(seed)\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.memory)\n",
    "        \n",
    "#     def add(self, states, actions, rewards, next_states, dones):\n",
    "#         \"\"\"\n",
    "#         Add trajecotries into replay buffer\n",
    "        \n",
    "#         Params\n",
    "#         =====\n",
    "#             states (numpy array) [num_agents, state_size]\n",
    "#             actions (numpy array) [num_agents, action_size]\n",
    "#             rewards (list) [num_agents]\n",
    "#             next_states (numpy array) [num_agents, state_size]\n",
    "#             dones (list) [num_agents]\n",
    "#         \"\"\"\n",
    "        \n",
    "#         e = (states, actions, rewards, next_states, dones)\n",
    "#         self.memory.append(e)\n",
    "        \n",
    "#     def sample(self):\n",
    "#         \"\"\"\n",
    "#         Sample the trajecoties and convert it to torch float tensor.\n",
    "        \n",
    "#         Returns\n",
    "#         ======\n",
    "#             Tuple of Torch Tensors :\n",
    "            \n",
    "#                 states_tensor, rewards_tensor, dones_tensor:\n",
    "#                 tensor's outermost dimension is num_agents, \n",
    "#                 and the second dimension is batch_size\n",
    "                \n",
    "#                 actions_tensor, next_states_tenosr:\n",
    "#                 outermost dimension is batch_size\n",
    "#                 and the second dimension is num_agents\n",
    "#         \"\"\"\n",
    "#         # list of tuples\n",
    "#         batch_list = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "#         # Convert the element into torch float tensors.\n",
    "        \n",
    "#         # batch of states [batch_size, num_agents, state_size]\n",
    "        \n",
    "#         #print(torch.from_numpy(np.vstack([i[0] for i in batch_list])).float().to(device).shape) # torch.Size([128, 24])\n",
    "#         states_tensor = torch.from_numpy(np.stack([i[0] for i in batch_list], axis=0)).float().to(device).permute(1,0,2)\n",
    "#         actions_tensor = torch.from_numpy(np.stack([i[1] for i in batch_list],axis=0)).float().to(device)\n",
    "#         rewards_tensor = torch.from_numpy(np.stack([i[2] for i in batch_list],axis=0)).float().to(device).permute(1,0)\n",
    "#         # preserve the shape [bactch_size, num_agents, state_size]\n",
    "#         next_states_tensor = torch.from_numpy(np.stack([i[3] for i in batch_list],axis=0)).float().to(device)\n",
    "#         # [bath_size, num_agents]\n",
    "#         dones_tensor = torch.from_numpy(np.stack([i[4] for i in batch_list],axis=0).astype(np.uint8)).float().to(device).permute(1,0)\n",
    "        \n",
    "#         return (states_tensor, actions_tensor, rewards_tensor, next_states_tensor, dones_tensor)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAU = 0.01\n",
    "LR = 0.001\n",
    "GAMMA = 0.95\n",
    "BUFFER_SIZE = int(1e6)\n",
    "BATCH_SIZE = 128\n",
    "TRAIN_FREQ = 10\n",
    "TRAIN_ITER = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG():\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, lr):\n",
    "        \"\"\"\n",
    "        Build model, random process and intilize it.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.local_actor = Actor(state_size, action_size, seed).to(device)\n",
    "        self.local_critic = Critic(state_size, action_size, seed).to(device)\n",
    "        self.target_actor = Actor(state_size, action_size, seed).to(device)\n",
    "        self.target_critic = Critic(state_size, action_size, seed).to(device)\n",
    "        \n",
    "        #self.replay_buffer = replay_buffer\n",
    "        \n",
    "        # Initialize target networks weights with local networks\n",
    "        self.hard_copy(self.local_actor, self.target_actor)\n",
    "        self.hard_copy(self.local_critic, self.target_critic)\n",
    "        \n",
    "        # Optimizer for local networks\n",
    "        self.actor_optimizer = torch.optim.Adam(self.local_actor.parameters(), lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.local_critic.parameters(), lr)\n",
    "        \n",
    "#     def act_local(self, state):\n",
    "#         \"\"\"\n",
    "#         select action with the local actor network.\n",
    "        \n",
    "#         Parmas\n",
    "#         ====\n",
    "#             state (torch Tensor) [state_size]\n",
    "#         Retuns\n",
    "#         ====\n",
    "#             action (torch Tensor) [action_size]\n",
    "#         \"\"\"\n",
    "#         self.local_actor.eval()\n",
    "#         with torch.no_grad():\n",
    "#             action = self.local_actor(state).cpu().data.numpy() + self.random_process.sample()\n",
    "#         self.local_actor.train()\n",
    "        \n",
    "#         return action\n",
    "    \n",
    "#     def act_target(self, state):\n",
    "#         \"\"\"\n",
    "#         select action with the target actor network\n",
    "        \n",
    "#         Parmas\n",
    "#         ====\n",
    "#             state (torch Tensor) [state_size]\n",
    "#         Retuns\n",
    "#         ====\n",
    "#             action (torch Tensor) [action_size]\n",
    "#         \"\"\"\n",
    "#         self.target_actor.eval()\n",
    "#         with torch.no_grad():\n",
    "#             action = self.target_actor(state)\n",
    "#         self.target_actor.train()\n",
    "        \n",
    "#         return action\n",
    "    \n",
    "#     def reset_noise(self):\n",
    "#         \"\"\"\n",
    "#         Reset the noise state every episode\n",
    "#         \"\"\"\n",
    "#         self.random_process.reset()\n",
    "    \n",
    "    def hard_copy(self, local, target):\n",
    "        \"\"\"\n",
    "        hard copy the weights of the local network to the target network\n",
    "        \"\"\"\n",
    "        for local_param, target_param in zip(local.parameters(), target.parameters()):\n",
    "            target_param.data.copy_(local_param.data)\n",
    "            \n",
    "    def soft_copy(self, tau):\n",
    "        \"\"\"\n",
    "        soft update target network\n",
    "        𝜃_target = 𝜏*𝜃_local + (1 - 𝜏)*𝜃_target\n",
    "        MADDPG alogrithm only update the actor networks with this technique. \n",
    "        \"\"\"\n",
    "        for local_param, target_param in zip(self.local_actor.parameters(), self.target_actor.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1-tau)*target_param.data)\n",
    "            \n",
    "        for local_param, target_param in zip(self.local_critic.parameters(), self.target_critic.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1-tau)*target_param.data)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.5394465 ,  0.09556682]),\n",
       " array([ 0.83691214,  0.53473487]),\n",
       " array([ 0.77496782,  0.23083627])]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=[np.random.rand(2), np.random.rand(2), np.random.rand(2)]\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5394465 ,  0.09556682],\n",
       "       [ 0.83691214,  0.53473487],\n",
       "       [ 0.77496782,  0.23083627]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(l, axis=0).reshape(3,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADDPG():\n",
    "    def __init__(self,num_agents, state_size, action_size, seed, lr=LR, tau=TAU, gamma=GAMMA):\n",
    "        \"\"\"\n",
    "        Intialize Multi Agents Class\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            num_agents (int) : # of agents\n",
    "            state_size (int)\n",
    "            action_size (int)\n",
    "            seed (int) : random seed\n",
    "            lr (float) : learning rate for optimizer\n",
    "            tau (float) : interpolation rate for soft update\n",
    "            \n",
    "         \"\"\"\n",
    "        self.num_agents = num_agents\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "#         self.agents_list = [] # list of multi-agents\n",
    "#         for i in range(num_agents):\n",
    "#             self.agents_list.append(DDPG(self.state_size, self.action_size, seed, LR))\n",
    "\n",
    "        self.agents_list = [DDPG(self.state_size, self.action_size, seed, LR) for _ in range(num_agents)]\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, seed) # shared replay buffer  \n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.noise = OUNoise(action_size, seed)\n",
    "        self.step_count = 0 # condition to decide whether to update networks.\n",
    "        \n",
    "    def act(self, states):\n",
    "        \"\"\"\n",
    "        Multi-agent select actions with the current policies and explorations.\n",
    "        a_i = µ_θ_i(o_i) +N_t\n",
    "        Params\n",
    "        =====\n",
    "            states (numpy array) [num_agents, state_size]\n",
    "        Retruns\n",
    "        ====\n",
    "            actions_array (numpy array) [num_agents, action_size]\n",
    "        \"\"\"\n",
    "#         action_list = []\n",
    "#         states_tensor = torch.from_numpy(states).float().to(device)\n",
    "        \n",
    "#         for i in range(self.num_agents):  \n",
    "#             action_list.append(self.agents_list[i].act_local(states_tensor[i]))\n",
    "\n",
    "#         # actions_tensor : [num_agents, action_size]    \n",
    "#         #actions_tensor = torch.cat(action_list, dim=1)\n",
    "#         #actions_array = actions_tensor.data.cpu().numpy()\n",
    "#         actions_array = np.concatenate(action_list).reshape(self.num_agents, -1)\n",
    "        \n",
    "        \n",
    "#         return actions_array\n",
    "        actions = np.zeros([2, 2])\n",
    "        for agent_idx, perspective_states in enumerate(states):\n",
    "            state_tensor = torch.from_numpy(perspective_states).float().to(device)\n",
    "                #self.agents_list[agent_idx].local_actor.device)\n",
    "\n",
    "            self.agents_list[agent_idx].local_actor.eval()\n",
    "            with torch.no_grad():\n",
    "                _actions = self.agents_list[agent_idx].local_actor.forward(\n",
    "                    state_tensor).cpu().data.numpy()\n",
    "            self.agents_list[agent_idx].local_actor.train()\n",
    "            actions[agent_idx, :] = _actions\n",
    "\n",
    "        # TODO: If mode is eval, epsilon is set to zero automatically and this\n",
    "        #  conditional is never triggered to add noise\n",
    "        \n",
    "        actions += self.noise.sample()\n",
    "        return np.clip(actions, -1, 1)\n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"\n",
    "        Store trajactories to shared replay buffer and do learning procedure.\n",
    "        \n",
    "        Params\n",
    "        =====\n",
    "            states (numpy array) [num_agents, state_size]\n",
    "            actions (numpy array) [num_agents, action_size]\n",
    "            rewards (numpy array) [num_agents]\n",
    "            next_states (numpy array) [num_agents, state_size]\n",
    "            dones (numpy array) [num_agent]\n",
    "        \"\"\"\n",
    "        # Add a trajactory into shared buffer\n",
    "        for i in range(self.num_agents):\n",
    "            self.memory.add(states[i], actions[i], rewards[i], next_states[i], dones[i])\n",
    "        \n",
    "        \n",
    "        # after every BATCH_SIZE smaples added to the replay buffer, network parameters are updated\n",
    "        self.step_count = (self.step_count + 1) % TRAIN_FREQ\n",
    "        # To perform learning, at least we need that memory size is larger than BATCH_SIZE\n",
    "        #if len(self.memory) > BATCH_SIZE:\n",
    "        if self.step_count == 0:\n",
    "            if len(self.memory) >= BATCH_SIZE:\n",
    "                for _ in range(TRAIN_ITER):\n",
    "                    self.learn()\n",
    "            \n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Update local actor and critic networks and target actor network.\n",
    "\n",
    "        \"\"\"\n",
    "        #[batch_size, state_size] 이런 식임\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample()\n",
    "        states_tensor = torch.from_numpy(states).float()\n",
    "        actions_tensor = torch.from_numpy(actions).float()\n",
    "        next_states_tensor = torch.from_numpy(next_states).float()\n",
    "        rewards_tensor = torch.from_numpy(rewards).float()\n",
    "        dones_tensor = torch.from_numpy(dones).float()\n",
    "\n",
    "        for agent in self.agents_list:\n",
    "            # states : [num_agents, batch_size state_size]\n",
    "            # actions : [batch_size, num_agents, actions_size]\n",
    "            # rewards : [num_agents, batch_size]\n",
    "            # next_states : [batch_size, num_agents, state_size]\n",
    "            # dones : [num_agents, batch_size]\n",
    "\n",
    "\n",
    "            # train the critic\n",
    "            #print(type(next_states_tensor))\n",
    "            #print(next_states_tensor.size())\n",
    "            future_return = agent.target_critic.forward(\n",
    "                next_states_tensor, agent.target_actor.forward(next_states_tensor))\n",
    "            \n",
    "            Q_target = rewards_tensor + (\n",
    "                    self.gamma * future_return.cpu() *\n",
    "                    (1-dones_tensor))\n",
    "\n",
    "            agent.local_critic.train()\n",
    "            Q_expected = agent.local_critic.forward(states_tensor, actions_tensor)\n",
    "            critic_loss = F.mse_loss(Q_expected, Q_target.to(device))\n",
    "\n",
    "            agent.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            \n",
    "            nn.utils.clip_grad_norm_(agent.local_critic.parameters(), 1)\n",
    "            agent.critic_optimizer.step()\n",
    "\n",
    "            # train the actor\n",
    "            # actions_pred = self.actor.forward(states_tensor)\n",
    "            actions_pred = agent.local_actor.forward(states_tensor)\n",
    "            agent.local_critic.eval()\n",
    "            actor_loss = -agent.local_critic.forward(states_tensor, actions_pred).mean()\n",
    "            agent.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            agent.actor_optimizer.step()\n",
    "            agent.local_critic.train()\n",
    "\n",
    "            # record for progress bar\n",
    "            agent.critic_loss_ = critic_loss.cpu().data.numpy()\n",
    "            agent.actor_loss_ = actor_loss.cpu().detach().numpy()\n",
    "\n",
    "            agent.soft_copy(tau=self.tau)\n",
    "\n",
    "       \n",
    "            self.noise.reset()\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------Update local critic------------------#\n",
    "#             self.agents_list[i].critic_optimizer.zero_grad()\n",
    "#             # a_prime_1, . . . , a_prime_N\n",
    "#             target_actions_list = []\n",
    "#             for j in range(self.num_agents):\n",
    "#                 target_action = self.agents_list[j].act_target(states[j]) #[batch_size, action_size]\n",
    "#                 target_actions_list.append(target_action)\n",
    "            \n",
    "#             # dim=0방향으로 하면 reshape때처럼 섞여서 끊김\n",
    "#             # Cocat target_actions_list dim=0 to [num_agents * batch_size ,action_size]\n",
    "#             # Cocat target_actions_list dim=1 to [batch_size ,num_agents *action_size]\n",
    "#             target_actions = torch.cat(target_actions_list, dim=1).to(device)\n",
    "#             # reshape target_actions into [batch_size, num_agents, action_size]\n",
    "#             target_actions = target_actions.view(-1, self.num_agents, self.action_size)\n",
    "            \n",
    "        \n",
    "#             # Calculate q_target with target critic network\n",
    "#             with torch.no_grad():\n",
    "#                 q_next = self.agents_list[i].target_critic(next_states, target_actions)\n",
    "#             #print((q_next* (1-dones[i])).shape) # torch.Size([128, 128])\n",
    "#             q_target = rewards[i] + self.gamma* q_next.view(1,-1) * (1-dones[i])\n",
    "#             q_value = self.agents_list[i].local_critic(states.permute(1,0,2), actions)\n",
    "            \n",
    "#             # input and target shapes do not match: input [128 x 1], target [128 x 128]\n",
    "#             critic_loss = F.mse_loss(q_value.view(-1),target=q_target.view(-1).detach())\n",
    "            \n",
    "#             critic_loss.backward()\n",
    "#             # Gradient clipping\n",
    "#             nn.utils.clip_grad_value_(self.agents_list[i].local_critic.parameters(),1.0)\n",
    "#             self.agents_list[i].critic_optimizer.step()\n",
    "            \n",
    "#             # ----------------Update local actor------------------#\n",
    "#             self.agents_list[i].actor_optimizer.zero_grad()\n",
    "#             pred_action = self.agents_list[i].local_actor(states[i]) # pred_action shape : [batch_size, action_size]\n",
    "#             actions = actions.permute(1,0,2)\n",
    "#             actions[i] = pred_action\n",
    "#             actor_loss = -self.agents_list[i].local_critic(states.permute(1,0,2), actions.permute(1,0,2)).mean()\n",
    "#             actor_loss.backward()\n",
    "#             self.agents_list[i].actor_optimizer.step()\n",
    "        \n",
    "#         # Update target nework\n",
    "#         for i in range(self.num_agents):\n",
    "#             self.agents_list[i].soft_copy(self.tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOAL_AVG_SCORE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "maddpg = MADDPG(num_agents, state_size, action_size, 3, LR, TAU, GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TennisBrain'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(num_episodes=2000, max_timestep=1000, print_every=50):\n",
    "    # for graph\n",
    "    total_score = []\n",
    "    # for calculating mean average score\n",
    "    score_window = deque(maxlen=100)\n",
    "    \n",
    "    for i in range(1,num_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        # [num_agets, state_size]\n",
    "        states = env_info.vector_observations\n",
    "        agents_scores = np.zeros(num_agents)\n",
    "        \n",
    "        for t in range(max_timestep):\n",
    "            # [num_agents, action_size]\n",
    "            actions = maddpg.act(states)\n",
    "            \n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            rewards = env_info.rewards\n",
    "            \n",
    "            maddpg.step(states, actions, rewards, next_states, dones)\n",
    "            agents_scores += rewards\n",
    "            states = next_states\n",
    "            if np.any(dones):\n",
    "                break\n",
    "        # Episode score\n",
    "        episode_score = max(agents_scores)\n",
    "        score_window.append(episode_score)\n",
    "        total_score.append(episode_score)\n",
    "        \n",
    "        if np.mean(score_window) >= GOAL_AVG_SCORE:\n",
    "            print('The number of episodes needed to solve the problem : {}'.format(i))\n",
    "        if (i% print_every) == 0:\n",
    "            print('Episode : {} \\t Current Score: {:.2f} \\t Average Score : {:.2f}'. format(i, episode_score, np.mean(score_window)))\n",
    "    return total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 50 \t Current Score: 0.00 \t Average Score : 0.00\n",
      "Episode : 100 \t Current Score: 0.00 \t Average Score : 0.01\n",
      "Episode : 150 \t Current Score: 0.10 \t Average Score : 0.01\n",
      "Episode : 200 \t Current Score: 0.10 \t Average Score : 0.04\n",
      "Episode : 250 \t Current Score: 0.50 \t Average Score : 0.10\n",
      "Episode : 300 \t Current Score: 0.20 \t Average Score : 0.31\n",
      "The number of episodes needed to solve the problem : 327\n",
      "The number of episodes needed to solve the problem : 328\n",
      "The number of episodes needed to solve the problem : 329\n",
      "The number of episodes needed to solve the problem : 330\n",
      "The number of episodes needed to solve the problem : 331\n",
      "The number of episodes needed to solve the problem : 332\n",
      "The number of episodes needed to solve the problem : 333\n",
      "The number of episodes needed to solve the problem : 334\n",
      "The number of episodes needed to solve the problem : 335\n",
      "The number of episodes needed to solve the problem : 336\n",
      "The number of episodes needed to solve the problem : 337\n",
      "The number of episodes needed to solve the problem : 338\n",
      "The number of episodes needed to solve the problem : 339\n",
      "The number of episodes needed to solve the problem : 340\n",
      "The number of episodes needed to solve the problem : 341\n",
      "The number of episodes needed to solve the problem : 342\n",
      "The number of episodes needed to solve the problem : 343\n",
      "The number of episodes needed to solve the problem : 344\n",
      "The number of episodes needed to solve the problem : 345\n",
      "The number of episodes needed to solve the problem : 346\n",
      "The number of episodes needed to solve the problem : 347\n",
      "The number of episodes needed to solve the problem : 348\n",
      "The number of episodes needed to solve the problem : 349\n",
      "The number of episodes needed to solve the problem : 350\n",
      "Episode : 350 \t Current Score: 0.70 \t Average Score : 0.62\n",
      "The number of episodes needed to solve the problem : 351\n",
      "The number of episodes needed to solve the problem : 352\n",
      "The number of episodes needed to solve the problem : 353\n",
      "The number of episodes needed to solve the problem : 354\n",
      "The number of episodes needed to solve the problem : 355\n",
      "The number of episodes needed to solve the problem : 356\n",
      "The number of episodes needed to solve the problem : 357\n",
      "The number of episodes needed to solve the problem : 358\n",
      "The number of episodes needed to solve the problem : 359\n",
      "The number of episodes needed to solve the problem : 360\n",
      "The number of episodes needed to solve the problem : 361\n",
      "The number of episodes needed to solve the problem : 362\n",
      "The number of episodes needed to solve the problem : 363\n",
      "The number of episodes needed to solve the problem : 364\n",
      "The number of episodes needed to solve the problem : 365\n",
      "The number of episodes needed to solve the problem : 366\n",
      "The number of episodes needed to solve the problem : 367\n",
      "The number of episodes needed to solve the problem : 368\n",
      "The number of episodes needed to solve the problem : 369\n",
      "The number of episodes needed to solve the problem : 370\n",
      "The number of episodes needed to solve the problem : 371\n",
      "The number of episodes needed to solve the problem : 372\n",
      "The number of episodes needed to solve the problem : 373\n",
      "The number of episodes needed to solve the problem : 374\n",
      "The number of episodes needed to solve the problem : 375\n",
      "The number of episodes needed to solve the problem : 376\n",
      "The number of episodes needed to solve the problem : 377\n",
      "The number of episodes needed to solve the problem : 378\n",
      "The number of episodes needed to solve the problem : 379\n",
      "The number of episodes needed to solve the problem : 380\n",
      "The number of episodes needed to solve the problem : 381\n",
      "The number of episodes needed to solve the problem : 382\n",
      "The number of episodes needed to solve the problem : 383\n",
      "The number of episodes needed to solve the problem : 384\n",
      "The number of episodes needed to solve the problem : 385\n",
      "The number of episodes needed to solve the problem : 386\n",
      "The number of episodes needed to solve the problem : 387\n",
      "The number of episodes needed to solve the problem : 388\n",
      "The number of episodes needed to solve the problem : 389\n",
      "The number of episodes needed to solve the problem : 390\n",
      "The number of episodes needed to solve the problem : 391\n",
      "The number of episodes needed to solve the problem : 392\n",
      "The number of episodes needed to solve the problem : 393\n",
      "The number of episodes needed to solve the problem : 394\n",
      "The number of episodes needed to solve the problem : 395\n",
      "The number of episodes needed to solve the problem : 396\n",
      "The number of episodes needed to solve the problem : 397\n",
      "The number of episodes needed to solve the problem : 398\n",
      "The number of episodes needed to solve the problem : 399\n",
      "The number of episodes needed to solve the problem : 400\n",
      "Episode : 400 \t Current Score: 0.10 \t Average Score : 0.57\n",
      "The number of episodes needed to solve the problem : 401\n",
      "The number of episodes needed to solve the problem : 402\n",
      "The number of episodes needed to solve the problem : 403\n",
      "The number of episodes needed to solve the problem : 404\n",
      "The number of episodes needed to solve the problem : 405\n",
      "The number of episodes needed to solve the problem : 406\n",
      "The number of episodes needed to solve the problem : 407\n",
      "The number of episodes needed to solve the problem : 408\n",
      "The number of episodes needed to solve the problem : 409\n",
      "The number of episodes needed to solve the problem : 410\n",
      "The number of episodes needed to solve the problem : 411\n",
      "The number of episodes needed to solve the problem : 412\n",
      "The number of episodes needed to solve the problem : 413\n",
      "The number of episodes needed to solve the problem : 414\n",
      "Episode : 450 \t Current Score: 0.10 \t Average Score : 0.44\n",
      "The number of episodes needed to solve the problem : 462\n",
      "The number of episodes needed to solve the problem : 463\n",
      "The number of episodes needed to solve the problem : 464\n",
      "The number of episodes needed to solve the problem : 465\n",
      "The number of episodes needed to solve the problem : 466\n",
      "The number of episodes needed to solve the problem : 467\n",
      "The number of episodes needed to solve the problem : 468\n",
      "The number of episodes needed to solve the problem : 469\n",
      "The number of episodes needed to solve the problem : 470\n",
      "The number of episodes needed to solve the problem : 471\n",
      "The number of episodes needed to solve the problem : 472\n",
      "The number of episodes needed to solve the problem : 473\n",
      "The number of episodes needed to solve the problem : 474\n",
      "The number of episodes needed to solve the problem : 475\n",
      "The number of episodes needed to solve the problem : 476\n",
      "The number of episodes needed to solve the problem : 477\n",
      "The number of episodes needed to solve the problem : 478\n",
      "The number of episodes needed to solve the problem : 479\n",
      "The number of episodes needed to solve the problem : 480\n",
      "The number of episodes needed to solve the problem : 481\n",
      "The number of episodes needed to solve the problem : 482\n",
      "The number of episodes needed to solve the problem : 483\n",
      "The number of episodes needed to solve the problem : 484\n",
      "The number of episodes needed to solve the problem : 485\n",
      "The number of episodes needed to solve the problem : 486\n",
      "The number of episodes needed to solve the problem : 487\n",
      "The number of episodes needed to solve the problem : 488\n",
      "The number of episodes needed to solve the problem : 489\n",
      "The number of episodes needed to solve the problem : 490\n",
      "The number of episodes needed to solve the problem : 491\n",
      "The number of episodes needed to solve the problem : 492\n",
      "The number of episodes needed to solve the problem : 493\n",
      "The number of episodes needed to solve the problem : 494\n",
      "The number of episodes needed to solve the problem : 495\n",
      "The number of episodes needed to solve the problem : 496\n",
      "The number of episodes needed to solve the problem : 497\n",
      "Episode : 500 \t Current Score: 0.20 \t Average Score : 0.50\n",
      "Episode : 550 \t Current Score: 0.09 \t Average Score : 0.36\n",
      "Episode : 600 \t Current Score: 0.10 \t Average Score : 0.40\n",
      "The number of episodes needed to solve the problem : 616\n",
      "The number of episodes needed to solve the problem : 617\n",
      "The number of episodes needed to solve the problem : 618\n",
      "The number of episodes needed to solve the problem : 619\n",
      "The number of episodes needed to solve the problem : 620\n",
      "The number of episodes needed to solve the problem : 621\n",
      "The number of episodes needed to solve the problem : 622\n",
      "The number of episodes needed to solve the problem : 623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of episodes needed to solve the problem : 624\n",
      "The number of episodes needed to solve the problem : 625\n",
      "The number of episodes needed to solve the problem : 626\n",
      "The number of episodes needed to solve the problem : 627\n",
      "The number of episodes needed to solve the problem : 628\n",
      "The number of episodes needed to solve the problem : 629\n",
      "The number of episodes needed to solve the problem : 630\n",
      "The number of episodes needed to solve the problem : 631\n",
      "The number of episodes needed to solve the problem : 632\n",
      "The number of episodes needed to solve the problem : 633\n",
      "The number of episodes needed to solve the problem : 634\n",
      "The number of episodes needed to solve the problem : 635\n",
      "The number of episodes needed to solve the problem : 636\n",
      "The number of episodes needed to solve the problem : 637\n",
      "The number of episodes needed to solve the problem : 638\n",
      "The number of episodes needed to solve the problem : 639\n",
      "The number of episodes needed to solve the problem : 640\n",
      "The number of episodes needed to solve the problem : 641\n",
      "The number of episodes needed to solve the problem : 642\n",
      "The number of episodes needed to solve the problem : 643\n",
      "The number of episodes needed to solve the problem : 644\n",
      "The number of episodes needed to solve the problem : 645\n",
      "The number of episodes needed to solve the problem : 646\n",
      "The number of episodes needed to solve the problem : 647\n",
      "The number of episodes needed to solve the problem : 648\n",
      "The number of episodes needed to solve the problem : 649\n",
      "The number of episodes needed to solve the problem : 650\n",
      "Episode : 650 \t Current Score: 0.10 \t Average Score : 0.70\n",
      "The number of episodes needed to solve the problem : 651\n",
      "The number of episodes needed to solve the problem : 652\n",
      "The number of episodes needed to solve the problem : 653\n",
      "The number of episodes needed to solve the problem : 654\n",
      "The number of episodes needed to solve the problem : 655\n",
      "The number of episodes needed to solve the problem : 656\n",
      "The number of episodes needed to solve the problem : 657\n",
      "The number of episodes needed to solve the problem : 658\n",
      "The number of episodes needed to solve the problem : 659\n",
      "The number of episodes needed to solve the problem : 660\n",
      "The number of episodes needed to solve the problem : 661\n",
      "The number of episodes needed to solve the problem : 662\n",
      "The number of episodes needed to solve the problem : 663\n",
      "The number of episodes needed to solve the problem : 664\n",
      "The number of episodes needed to solve the problem : 665\n",
      "The number of episodes needed to solve the problem : 666\n",
      "The number of episodes needed to solve the problem : 667\n",
      "The number of episodes needed to solve the problem : 668\n",
      "The number of episodes needed to solve the problem : 669\n",
      "The number of episodes needed to solve the problem : 670\n",
      "The number of episodes needed to solve the problem : 671\n",
      "The number of episodes needed to solve the problem : 672\n",
      "The number of episodes needed to solve the problem : 673\n",
      "The number of episodes needed to solve the problem : 674\n",
      "The number of episodes needed to solve the problem : 675\n",
      "The number of episodes needed to solve the problem : 676\n",
      "The number of episodes needed to solve the problem : 677\n",
      "The number of episodes needed to solve the problem : 678\n",
      "The number of episodes needed to solve the problem : 679\n",
      "The number of episodes needed to solve the problem : 680\n",
      "The number of episodes needed to solve the problem : 681\n",
      "The number of episodes needed to solve the problem : 682\n",
      "The number of episodes needed to solve the problem : 683\n",
      "The number of episodes needed to solve the problem : 684\n",
      "The number of episodes needed to solve the problem : 685\n",
      "The number of episodes needed to solve the problem : 686\n",
      "The number of episodes needed to solve the problem : 687\n",
      "The number of episodes needed to solve the problem : 688\n",
      "The number of episodes needed to solve the problem : 689\n",
      "The number of episodes needed to solve the problem : 690\n",
      "The number of episodes needed to solve the problem : 691\n",
      "The number of episodes needed to solve the problem : 692\n",
      "The number of episodes needed to solve the problem : 693\n",
      "The number of episodes needed to solve the problem : 694\n",
      "The number of episodes needed to solve the problem : 695\n",
      "The number of episodes needed to solve the problem : 696\n",
      "The number of episodes needed to solve the problem : 697\n",
      "The number of episodes needed to solve the problem : 698\n",
      "The number of episodes needed to solve the problem : 699\n",
      "The number of episodes needed to solve the problem : 700\n",
      "Episode : 700 \t Current Score: 0.20 \t Average Score : 0.60\n",
      "The number of episodes needed to solve the problem : 701\n",
      "The number of episodes needed to solve the problem : 702\n",
      "The number of episodes needed to solve the problem : 703\n",
      "The number of episodes needed to solve the problem : 704\n",
      "The number of episodes needed to solve the problem : 705\n",
      "The number of episodes needed to solve the problem : 706\n",
      "The number of episodes needed to solve the problem : 707\n",
      "The number of episodes needed to solve the problem : 708\n",
      "The number of episodes needed to solve the problem : 709\n",
      "The number of episodes needed to solve the problem : 710\n",
      "The number of episodes needed to solve the problem : 711\n",
      "The number of episodes needed to solve the problem : 712\n",
      "The number of episodes needed to solve the problem : 713\n",
      "The number of episodes needed to solve the problem : 714\n",
      "The number of episodes needed to solve the problem : 715\n",
      "The number of episodes needed to solve the problem : 716\n",
      "The number of episodes needed to solve the problem : 717\n",
      "The number of episodes needed to solve the problem : 718\n",
      "The number of episodes needed to solve the problem : 719\n",
      "The number of episodes needed to solve the problem : 720\n",
      "The number of episodes needed to solve the problem : 721\n",
      "The number of episodes needed to solve the problem : 722\n",
      "The number of episodes needed to solve the problem : 723\n",
      "The number of episodes needed to solve the problem : 724\n",
      "The number of episodes needed to solve the problem : 725\n",
      "The number of episodes needed to solve the problem : 726\n",
      "The number of episodes needed to solve the problem : 727\n",
      "The number of episodes needed to solve the problem : 728\n",
      "The number of episodes needed to solve the problem : 729\n",
      "The number of episodes needed to solve the problem : 730\n",
      "The number of episodes needed to solve the problem : 731\n",
      "The number of episodes needed to solve the problem : 732\n",
      "The number of episodes needed to solve the problem : 733\n",
      "The number of episodes needed to solve the problem : 734\n",
      "The number of episodes needed to solve the problem : 735\n",
      "The number of episodes needed to solve the problem : 736\n",
      "The number of episodes needed to solve the problem : 737\n",
      "The number of episodes needed to solve the problem : 738\n",
      "The number of episodes needed to solve the problem : 739\n",
      "The number of episodes needed to solve the problem : 740\n",
      "The number of episodes needed to solve the problem : 741\n",
      "The number of episodes needed to solve the problem : 742\n",
      "The number of episodes needed to solve the problem : 743\n",
      "The number of episodes needed to solve the problem : 744\n",
      "The number of episodes needed to solve the problem : 745\n",
      "The number of episodes needed to solve the problem : 746\n",
      "The number of episodes needed to solve the problem : 747\n",
      "The number of episodes needed to solve the problem : 748\n",
      "The number of episodes needed to solve the problem : 749\n",
      "The number of episodes needed to solve the problem : 750\n",
      "Episode : 750 \t Current Score: 0.00 \t Average Score : 0.59\n",
      "The number of episodes needed to solve the problem : 751\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-180-e03c1e13c065>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtotal_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-179-1b059daa4b6a>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(num_episodes, max_timestep, print_every)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mmaddpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0magents_scores\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-174-18c004b25402>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, states, actions, rewards, next_states, dones)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_ITER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-174-18c004b25402>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \"\"\"\n\u001b[1;32m    103\u001b[0m         \u001b[0;31m#[batch_size, state_size] 이런 식임\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mstates_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mactions_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-151-756c5dc0b27c>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m              if transition is not None])\n\u001b[1;32m     36\u001b[0m         actions = np.vstack(\n\u001b[0;32m---> 37\u001b[0;31m             [transition.action for transition in experiences\n\u001b[0m\u001b[1;32m     38\u001b[0m              if transition is not None])\n\u001b[1;32m     39\u001b[0m         rewards = np.vstack(\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_2d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_score = training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f20fe56bf98>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig = plt.figure()\n",
    "ax=fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(total_score)), total_score)\n",
    "plt.xlabel('# of episodes')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
